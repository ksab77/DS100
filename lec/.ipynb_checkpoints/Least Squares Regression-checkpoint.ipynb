{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context(\"talk\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=False)\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "# The polling here is to ensure that plotly.js has already been loaded before\n",
    "# setting display alignment in order to avoid a race condition.\n",
    "display(HTML(\n",
    "    '<script>'\n",
    "        'var waitForPlotly = setInterval( function() {'\n",
    "            'if( typeof(window.Plotly) !== \"undefined\" ){'\n",
    "                'MathJax.Hub.Config({ SVG: { font: \"STIX-Web\" }, displayAlign: \"center\" });'\n",
    "                'MathJax.Hub.Queue([\"setRenderer\", MathJax.Hub, \"SVG\"]);'\n",
    "                'clearInterval(waitForPlotly);'\n",
    "            '}}, 5000 );'\n",
    "    '</script>'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.figure_factory as ff\n",
    "import cufflinks as cf\n",
    "\n",
    "cf.set_config_file(offline=False, world_readable=True, theme='ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Least Squares Linear Regression\n",
    "\n",
    "The **linear model** is a generalization of our earlier two dimensional $y = m x + b$ model:\n",
    "\n",
    "$$ \\large\n",
    "f_\\theta(x) = \\sum_{j=1}^p \\theta_j x_j\n",
    "$$\n",
    "\n",
    "**Note:**\n",
    "1. This is the model for a **single data point** $x$\n",
    "1. The data point is **$p$-dimensional**\n",
    "1. The subscript $j$ is indexing each of the $p$ dimensions\n",
    "\n",
    "To simplify the presentation we will use the following vector notation:\n",
    "\n",
    "$$ \\large\n",
    "f_\\theta(x) =  x^T \\theta\n",
    "$$\n",
    "\n",
    "You can see this in the following figure:\n",
    "\n",
    "<img src=\"images/vector_dot.png\" width=\"400px\">\n",
    "\n",
    "As we will see, shortly, this is a **very expressive parametric model**.  \n",
    "\n",
    "In previous lectures we derived the **Least Squares** parameter values for this model.  Let's derive them once more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Notation\n",
    "\n",
    "Before we proceed we need to define some new notation to describe the entire dataset.  We introduce the design (covariate) matrix $X$ and the response matrix (vector) $Y$ which encode the data:\n",
    "\n",
    "<img src=\"images/design_matrix.png\" width=\"400px\">\n",
    "\n",
    "**Notes:**\n",
    "1. The **rows** of $X$ correspond to records (e.g., users in our database)\n",
    "1. The **columns** of $X$ correspond to **features** (e.g., the age, income, height).\n",
    "1. **CS and Stats Terminology Issue (p=d):** In statistics we use $p$ to denote the number of columns in $X$ which corresponds to the number of *parameters* in the corresponding linear model.  In computer science we use $d$ instead of $p$ to refer to the number of columns.  The $d$ is in reference to the *dimensionality* of each record.  You can see the differences in focus on the model and the data. If you get confused just flip the paper (or your computer) upside down and it will make more sense.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Least Squares Loss Objective\n",
    "\n",
    "We can write the loss using the matrix notation:\n",
    "\n",
    "\\begin{align}\n",
    "\\large L_D(\\theta) \n",
    "& \\large = \n",
    "\\frac{1}{n}\\sum_{i=1}^n \\left(Y_i -  f_\\theta(X_i)\\right)^2 \\\\\n",
    "& \\large = \n",
    "\\frac{1}{n}\\sum_{i=1}^n \\left(Y_i -  X_i^T \\theta \\right)^2 \\\\\n",
    "& \\large = \n",
    "\\frac{1}{n}\\left(Y -  X \\theta \\right)^T \\left(Y -  X \\theta \\right) \n",
    "\\end{align}\n",
    "\n",
    "Note that the last line $X_i \\theta$ is the dot product of the \n",
    "\n",
    "<img src=\"images/matrix_dot.png\" width=\"400px\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expanding the Loss\n",
    "\n",
    "We can further simply the last expression by expanding the product:\n",
    "\n",
    "\\begin{align} \\large\n",
    "L_D(\\theta) \n",
    "& \\large = \n",
    "\\frac{1}{n}\\left(Y -  X \\theta \\right)^T \\left(Y -  X \\theta \\right) \\\\\n",
    "& \\large = \n",
    "\\frac{1}{n}\\left(Y^T -  \\left(X \\theta\\right)^T \\right) \\left(Y -  X \\theta \\right) \\\\\n",
    "& \\large = \n",
    "\\frac{1}{n}\\left(Y^T \\left(Y -  X \\theta \\right) -  \\left(X \\theta\\right)^T \\left(Y -  X \\theta \\right) \\right) \\\\\n",
    "& \\large  = \n",
    "\\frac{1}{n}\\left( \n",
    "   Y^T Y -  Y^T X \\theta  - \\left(X \\theta \\right)^T Y + \\left(X \\theta \\right)^T  X \\theta \n",
    "\\right) \\\\\n",
    "& \\large =\n",
    "\\frac{1}{n} \\left( \n",
    " Y^T Y -  2 Y^T X \\theta + \\theta^T  X^T  X \\theta \n",
    "\\right)\n",
    "\\end{align}\n",
    "\n",
    "**Note:** Because $\\left(X \\theta \\right)^T Y$ is a scalar it is equal to its transpose $Y^T X \\theta$ \n",
    "and therefore $- Y^T X \\theta  - \\left(X \\theta \\right)^T Y  = -2 Y^T X \\theta$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Loss Minimizing $\\theta$ \n",
    "Recall our goal is to compute:\n",
    "\n",
    "\\begin{align} \\large\n",
    "\\hat{\\theta} = \\arg \\min_\\theta L(\\theta)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "Which we can compute by taking the **gradient** of the loss and setting it equal to zero.\n",
    "\n",
    "\n",
    "\\begin{align} \\large\n",
    "\\nabla_\\theta L(\\theta) \n",
    "& \\large =\n",
    "\\frac{1}{n} \\left( \n",
    " \\nabla_\\theta Y^T Y -  \\nabla_\\theta 2 Y^T X \\theta + \\nabla_\\theta \\theta^T  X^T  X \\theta \n",
    "\\right)\n",
    "\\end{align}\n",
    "\n",
    "To complete this expression we will need some basic gradient identities.\n",
    "\n",
    "The first two identities we will use:\n",
    "\n",
    "1. $\\large \\nabla_\\theta \\left( A \\right) = 0$ which allows us to compute $\\large \\nabla_\\theta Y^T Y = 0$\n",
    "1. $\\large \\nabla_\\theta \\left( A \\theta  \\right) = A^T$ which allows us to compute $\\large \\nabla_\\theta 2 Y^T X \\theta = \\left( 2 Y^T X \\right)^T = 2 X^T Y$\n",
    "\n",
    "The remaining quadratic gradient identity:\n",
    "1. $\\large \\nabla_\\theta \\left( \\theta^T A \\theta \\right) = A\\theta + A^T \\theta$ allows us to compute:\n",
    "$$\n",
    "\\large\n",
    "\\nabla_\\theta \\theta^T  X^T  X \\theta = X^T  X  \\theta + (X^T  X)^T \\theta = 2 X^T X \\theta\n",
    "$$\n",
    "\n",
    "Thus we get the final gradient value: \n",
    "\n",
    "\\begin{align} \\large\n",
    "\\nabla_\\theta L(\\theta) \n",
    "& \\large =\n",
    "\\frac{1}{n} \\left( \n",
    " \\nabla_\\theta Y^T Y -  \\nabla_\\theta 2 Y^T X \\theta + \\nabla_\\theta \\theta^T  X^T  X \\theta \n",
    "\\right)\n",
    "\\\\\n",
    "& \\large =\n",
    "\\frac{1}{n} \\left( \n",
    " 0 -  2 X^T Y  +  2 X^T  X \\theta \n",
    "\\right) \n",
    "\\end{align}\n",
    "\n",
    "Setting the gradient equal to zero we get the famous **Normal Equations**:\n",
    "\n",
    "$$\\large\n",
    "X^T  X \\theta =  X^T Y\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\\large\n",
    " \\theta = \\left(X^T  X\\right)^{-1} X^T Y\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br/><br/><br/><br/>\n",
    "\n",
    "\n",
    "# The Normal Equation\n",
    "\n",
    "The normal equations define the least squares optimal parameter value  $\\hat{\\theta}$ for the linear regression model:\n",
    "\n",
    "$$\\large\n",
    " \\hat{\\theta{}} = \\left(X^T  X\\right)^{-1} X^T Y\n",
    "$$\n",
    "\n",
    "and have the pictorial interpretation of the matrices:\n",
    "\n",
    "<img src=\"images/normal_equations.png\" width=\"400px\">\n",
    "\n",
    "It is worth noting that for common settings where $n >> p$ (i.e., there are many records and only a few dimensions) the matrix $X^T X$ and $X^T Y$ are small relative to the data.  That is they summarize the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toy Data\n",
    "\n",
    "For this exercise we are going to use synthetic data to illustrate the basic ideas of model design. Notice here that we are generating data from a linear model with Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"data/toy_training_data.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data ---------------------\n",
    "train_points = go.Scatter(name = \"Training Data\", \n",
    "                          x = train_data['X'], y = train_data['Y'], \n",
    "                          mode = 'markers')\n",
    "# layout = go.Layout(autosize=False, width=800, height=600)\n",
    "py.iplot(go.Figure(data=[train_points]), \n",
    "         filename=\"L19_b_p1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving the Normal Equations\n",
    "\n",
    "There are several ways to compute the normal equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the Data as Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we select a list of columns to return a matrix (n,p)\n",
    "X = train_data[['X']].values\n",
    "print(\"X shape:\", X.shape)\n",
    "\n",
    "Y = train_data['Y'].values\n",
    "print(\"Y shape:\", Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the ones column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phiX = np.hstack([X, np.ones(X.shape)])\n",
    "phiX[1:5,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving for $\\hat{\\theta{}}$\n",
    "\n",
    "There are multiple ways to solve for $\\hat{\\theta{}}$. Following the solution to the normal equations:\n",
    "\n",
    "$$\\large\n",
    " \\hat{\\theta{}} = \\left(X^T  X\\right)^{-1} X^T Y\n",
    "$$\n",
    "\n",
    "we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat = np.linalg.inv(phiX.T @ phiX) @ phiX.T @ Y\n",
    "theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However computing inverting and multiplying (i.e., solving) can be accomplished with a special routine more efficiently:\n",
    "\n",
    "$$\\large\n",
    "A^{-1}b = \\texttt{solve}(A, b)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_hat = np.linalg.solve(phiX.T @ phiX, phiX.T @ Y)\n",
    "theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Package to estimate the linear model\n",
    "\n",
    "In practice, it is generally better to use specialized software packages for linear regression.  In Python, [scikit-learn](http://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares) is the standard package for regression.  \n",
    "\n",
    "![scikit-learn logo](http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png)\n",
    "\n",
    "Here we will take a very brief tour of how to use scikit-learn for regression.  Over the next few weeks we will use scikit-learn for a range of different task.\n",
    "\n",
    "You can use the the [scikit-learn `linear_model` package](http://scikit-learn.org/stable/modules/linear_model.html) to compute the normal equations. This package supports a wide range of **generalized linear models**.  For those who are interested in studying machine learning, I would encourage you to skim through the descriptions of the various models in the `linear_model` package. These are the foundation of most practical applications of supervised machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Intercept Term**  Scikit-learn can automatically add the intercept term.  This can be helpful since you don't need to remember to add it later when making a prediction.  In the following we create a model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "line_reg = linear_model.LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then fit the model in one line (this solves the normal equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the data\n",
    "line_reg.fit(train_data[['X']], train_data['Y'])\n",
    "np.hstack([line_reg.coef_, line_reg.intercept_]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the solution\n",
    "\n",
    "In the following we plot the solution along with it's residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions Using Linear Algebra\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making predictions at each of the training data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = phiX @ theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the trained model to render predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_hat = line_reg.predict(train_data[['X']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the fit \n",
    "\n",
    "To visualize the fit line we will make a set of predictions at 10 evenly spaced points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_query = np.linspace(train_data['X'].min()-1, train_data['X'].max() +1, 1000)\n",
    "phiX_query = np.hstack([X_query[:,np.newaxis], np.ones((len(X_query),1))])\n",
    "y_query_pred = phiX_query @ theta_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then plot the residuals along with the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the least squares regression line \n",
    "basic_line = go.Scatter(name = r\"Linear Model\", x=X_query, y = y_query_pred)\n",
    "\n",
    "# Definethe residual lines segments, a separate line for each \n",
    "# training point\n",
    "residual_lines = [\n",
    "    go.Scatter(x=[x,x], y=[y,yhat],\n",
    "               mode='lines', showlegend=False, \n",
    "               line=dict(color='black', width = 0.5))\n",
    "    for (x, y, yhat) in zip(train_data['X'], train_data['Y'], y_hat)\n",
    "]\n",
    "\n",
    "# Combine the plot elements\n",
    "py.iplot([train_points, basic_line] + residual_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examining the Residuals\n",
    "\n",
    "It is often helpful to examine the residuals.  Ideally the residuals should be normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = train_data['Y'] - y_hat\n",
    "sns.distplot(residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might also plot $\\hat{Y}$ vs $Y$.  Ideally, the data should be along the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot.ly plotting code\n",
    "py.iplot(go.Figure(\n",
    "    data = [go.Scatter(x=train_data['Y'], y=y_hat, mode='markers', name=\"Points\"),\n",
    "            go.Scatter(x=[-10, 50], y = [-10, 50], name=\"Diagonal\")],\n",
    "    layout = dict(title=r\"Y_hat vs Y Plot\", xaxis=dict(title=\"Y\"), \n",
    "                  yaxis=dict(title=r\"$\\hat{Y}$\"))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of the residuals should be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting against dimensions in X\n",
    "\n",
    "When plotted against any one dimension we don't want to see any patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot.ly plotting code\n",
    "py.iplot(go.Figure(\n",
    "    data = [go.Scatter(x=train_data['X'], y=residuals, mode='markers')],\n",
    "    layout = dict(title=\"Residual Plot\", xaxis=dict(title=\"X\"), \n",
    "                  yaxis=dict(title=\"Residual\"))\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying Kernel regression we can look for patterns in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.kernel_ridge import KernelRidge\n",
    "\n",
    "# Use Kernelized Ridge Regression with Radial Basis Functions to \n",
    "# compute a smoothed estimator.  Later in this notebook we will \n",
    "# actually implement part of this  ...\n",
    "clf = KernelRidge(kernel='rbf', alpha=2)\n",
    "clf.fit(train_data[['X']], residuals)\n",
    "residual_smoothed = clf.predict(np.reshape(X_query, (len(X_query),1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the residuals with with a kernel smoothing curve\n",
    "py.iplot(go.Figure(\n",
    "    data = [go.Scatter(name = \"Residuals\", \n",
    "                       x=train_data['X'], y=residuals, \n",
    "                       mode='markers'),\n",
    "            go.Scatter(name = \"Smoothed Approximation\", \n",
    "                       x=X_query, y=residual_smoothed, \n",
    "                       line=dict(dash=\"dash\"))],\n",
    "    layout = dict(title=\"Residual Plot\", xaxis=dict(title=\"X\"), \n",
    "                  yaxis=dict(title=\"Residual\"))\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Normal Equations when $n$ is big"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve the normal equations we need to compute the following:\n",
    "\n",
    "$$\\large\n",
    "A = X^T X = \\sum_{i=1}^n X_i^T X_i = \n",
    "\\sum_{i=1}^n \\left(\\begin{matrix}\n",
    "X_{i,1}^2 & X_{i,1}  X_{i,2}  & \\ldots & X_{i,1}  X_{i,p} \\\\\n",
    "X_{i,2} X_{i,1} & X_{i,2}^2   & \\ldots & X_{i,2}  X_{i,p} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "X_{i,p} X_{i,1} & X_{i,p} X_{i,2}   & \\ldots & X_{i,p}^2 \n",
    "\\end{matrix}\\right) \\in \\mathbb{R}^{p \\times p}\n",
    "$$\n",
    "\n",
    "$$\\large\n",
    "b = X^T Y = \\sum_{i=1}^n X_i^T Y_i = \\sum_{i=1}^n \n",
    "\\left(\\begin{matrix}\n",
    "X_{i,1} Y_i \\\\\n",
    "X_{i,2} Y_i \\\\\n",
    "\\vdots \\\\\n",
    "X_{i,p} Y_i\n",
    "\\end{matrix}\\right) \\in \\mathbb{R}^{p}\n",
    "$$\n",
    "\n",
    "Note that both $A$ and $b$ are order $p$ and they can be written as a **sum** over $n$.  In the homework you will have the opportunity to compute these in map reduce.  Once $A$ and $b$ are computed we can then use `np.linalg.solve` to complete the calculation locally on our computer since we are assuming $p$ is relatively small.  If $p$ is not small than we will need to modify the model and the solver.  More on this in future lectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Calculation\n",
    "\n",
    "Here we simulate distributed computing library by using the built-in `sum` and `map` operations in Python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "XtX = sum(map(lambda x: np.outer(x, x),  phiX))\n",
    "XtY = sum(map(lambda xy: xy[0] * xy[1],  zip(phiX, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Calculation\n",
    "\n",
    "The values of $X^T X$ and $X^T Y$ are small:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"XtX:\\n\", XtX)\n",
    "print(\"XtY:\\n\", XtY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.solve(XtX, XtY)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "322px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
